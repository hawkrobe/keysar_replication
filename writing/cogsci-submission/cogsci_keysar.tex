% arara: pdflatex
% arara: bibtex
% arara: pdflatex
% arara: pdflatex
% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.
 
\documentclass[10pt,letterpaper]{article}
 
\usepackage{hyperref}
\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{color}
\usepackage{todonotes}
\usepackage{dsfont}
\usepackage{array}
\usepackage{textcomp}
\usepackage{multirow}

\definecolor{Red}{RGB}{255,0,0}
\newcommand{\red}[1]{\textcolor{Red}{#1}}

\newcommand{\jd}[1]{\green{$^*$}\marginpar{\footnotesize{JD: \green{#1}}}}
\definecolor{Green}{RGB}{10,200,100}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}


\newcommand{\subsubsubsection}[1]{{\em #1}}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\tableref}[1]{Table \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\appref}[1]{Appendix \ref{#1}}
\newcommand{\sectionref}[1]{Section \ref{#1}}

\title{Conversational expectations account for apparent limits on theory of mind use}
 
\author{{\large \bf Robert X.~D.~Hawkins, Noah D.~Goodman}\\
  \{rxdh,ngoodman\}@stanford.edu\\
  Department of Psychology, 450 Serra Mall \\
  Stanford, CA 94305 USA}


\begin{document}

\maketitle

\begin{abstract}
People are capable of accurately reasoning about the mental states of other people using theory of mind. An influential series of language understanding experiments by Keysar and colleagues, however, showed that adults systematically failed to take the speaker's perspective into account. This has been widely interpreted as a limitation on theory of mind use. In this paper we argue that these apparent failures are in fact successes. Through a minimal pair of replications comparing scripted vs. unscripted speakers, we show that critical utterances used by Keysar and colleagues are \emph{uncooperative}: they are less informative than what a speaker would actually produce in that situation. A listener who correctly reasons about the pragmatics of the task, thereby \emph{using} theory of mind, will rationally ignore whether an object is hidden to the speaker or not and will instead rely on the precision of the utterance to disambiguate. When an uncooperative confederate flouts these expectations, this strategy produces errors; when we allow participants to naturally interact, this strategy is justified and errors are reduced.

\textbf{Keywords:} 
Theory of mind; social cognition; pragmatics
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure*}[t!]
\begin{center}
\includegraphics[scale = .5]{images/overall_view.png}
\end{center}
\vspace{-.25cm}
\caption{Interface used in the reported experiments. Objects behind the black squares were hidden from the director.}
\label{fig:interface}
\end{figure*}

Humans can accurately and intelligently reason about the mental states of other humans. Among other things, this ability -- called \emph{theory of mind} \cite{PremackWoodruff78_ChimpanzeeToM} -- allows us to infer the underlying beliefs and intentions that motivate others' actions, and to use these inferences to predict future actions \cite{BakerSaxeTenenbaum09_ActionUnderstandingInversePlanning}. Children acquire this ability by at least age six \cite{WimmerPerner83_BeliefsAboutBeliefs, WellmanCrossWatson01_ToMMetaAnalysis} and it serves as an important landmark in the developmental trajectory of intuitive theory use \cite{GopnikWellman12_ReconstructingConstructivism}.
%
While theory of mind use often appears to be automatic and effortless, Keysar and colleagues \cite{KeysarBarr___Brauner00_TakingPerspective, KeysarLinBarr03_LimitsOnTheoryOfMindUse, LinKeysarEpley10_ReflexivelyMindblind} have argued that it is actually the opposite, even for adults: we are ``mindblind" by default and only overcome our egocentric biases through an effortful process of perspective-taking. In other words, while adults are \emph{capable} of applying theory of mind reasoning, we do not always apply it reliably. 

In this paper we argue that the apparent failures used to support this view are in fact successes for sophisticated social reasoning. In particular, we argue that critical utterances used by Keysar and colleagues are \emph{uncooperative}: they are less informative than what a speaker would actually produce in that situation; listeners who are sensitive to the pragmatics of the situation expect these more informative utterances and produce ``errors'' when their expectations are flouted. 

The argument of Keysar and colleagues is based on an elegant experimental paradigm, where participants played a simple communication game with a confederate. The two players were placed on opposite sides of a $4 \times 4$ grid containing a set of everyday objects (see Fig. \ref{fig:interface}). The confederate played the role of `director,' giving instructions about how to move objects around a grid, and the participant played the role of `matcher,' attempting to follow these instructions. For example, the objects in one trial included a cassette tape. The director gave an instruction like `move the tape up one square,' referring to the cassette.
Critically, some objects were occluded such that only the matcher could see them, creating an asymmetry in the players' knowledge. To perform accurately on critical trials, the matcher would need to apply theory of mind to reason about the director's beliefs. For example, imagine a much larger measuring cup were placed in an occluded slot: if a participant failed to account for the director's (partial) knowledge, she might interpret `the large cup' to mean this occluded cup (which the director couldn't possibly know about). 
Indeed, \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} found that participants attempted to move the hidden item in 30\% of cases: 71\% of participants attempted to move this hidden item at least once (out of four critical cases) in the experimental condition, compared to 0\% in a control condition where there was no ambiguity over the referent. 
Additionally, eye-tracking data showed that participants considered the hidden item more often and for longer in the experimental condition than the control condition. 

While these results are compelling, the paradigm has been criticized from several different angles. 
First, \citeA{HellerGrodnerTanenhaus08_Perspective} has pointed out that in many cases, the hidden object was a better fit for the referring expression than the one in common ground (e.g. the hidden roll of tape vs. the cassette tape for ``the tape''), making the hidden object \emph{a priori} more likely to be the referent. Although relative fit was not empirically evaluated prior to our study, it would generally be fairer to compare two objects that fit the referring expression equally well. 
Second, the viewpoint asymmetry paradigm is somewhat unnatural: common ground is typically built incrementally over the course of an interaction rather than presented all at once, and it is rare for a shared display to differ in perceptual accessibility \cite{HannaTanenhausTrueswell03_CommonGroundPerspective}. 
Third, none of the experiments reported by Keysar and colleagues included a key comparison condition where the critical item (e.g. the largest measuring cup) was \emph{also} in common ground \cite{BrownSchmidtHanna11_IncrementalPerspectiveTaking}---perhaps people would refer to the critical item less in the privileged condition than in the full common ground condition, which would put a more positive spin on the results. %Note that none of these criticisms invalidate the paradoxical result that listeners move an occluded object, they just suggest an alternative explanation. Reference disambiguation is probabilistic: common ground and goodness of fit are weighed against one another.

In this paper, we offer an additional factor that helps account for Keysar's results. 
Theory of mind as applied within language understanding depends on an accurate model of what a speaker would say in different situations; given an utterance, a listener can then reason backward to the most plausible situation \cite{FrankGoodman12_PragmaticReasoningLanguageGames, GoodmanStuhlmuller13_KnowledgeImplicature}.
This suggests that we consider whether the utterances produced by the confederate in Keysar's critical conditions were actually what a speaker in that context would be expected to say. 
If not, then perhaps listeners are making choices that are in fact consistent with a correct pragmatic interpretation of the confederate's (uncooperative) utterance.
More precisely, when both players know that objects are occluded in the display, the speaker may tend to add additional precision to references in order to avoid confusion. If the listener \emph{expects} the speaker to do this, they will pragmatically pick the \emph{a priori} more likely referent of the referring expression, which in critical trials will be the occluded object. In other words, it is precisely \emph{because} the listener takes the speaker's mental state into consideration that they are tricked by an uncooperative confederate into choosing the wrong item. 

We began by replicating \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} in a multi-player web experiment. We recruited participants to be both director and matcher (instead of using a confederate), but instructions for critical items, as well as a random subset of filler items, remained scripted as in the original study. We replicated the original finding, but noted a tendency of directors to be overinformative in unscripted trials. We then ran the same experiment without scripting the directions for critical trials, observing unconstrained director behavior. We observe much greater precision in unconstrained director utterances, which match targets much better than distractors, and better performance of the matchers. This minimal pair of experiments demonstrates that listener mistakes are at least partially due to the pragmatics of the task, ironically showing that apparent failures of theory of mind are in fact attributable to sophisticated expectations about speaker behavior---that is, to theory of mind.

\begin{table*}
\begin{center}
\begin{tabular}{ p{2cm} | r | r |  r || r | r | r || r | r | r}
& \multicolumn{3}{c||}{\% attempted at least once} & \multicolumn{3}{c||}{\% attempted at least twice} & \multicolumn{3}{c}{\% of total cases}\\
\hline
& Orig. & Exp. 1 & Exp. 2 & Orig. & Exp. 1 & Exp. 2 & Orig. & Exp. 1 & Exp. 2  \\
Experimental & 71 & 93 & 61 & 46 & 57 & 32 & 30 & 43 & 24\\
Baseline        & 0   & 7   & 0   & 0   & 0   & 0   & 0   & 2   & 0\\
\end{tabular}
\caption{Side-by-side comparison of error rates in \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} and our two replications. The first section shows the percentage of \emph{participants} attempting to move the occluded distractor at least one of the four possible cases. The second section shows the percentage attempting to move it at least twice. The third section shows the percentage of \emph{all experimental trials} that the participant actually tried to move the occluded object. }
\label{table:mainResults}
\end{center}
\end{table*}

\section{Exp.~1: Scripted Replication}
\label{sec:Exp1}

\subsection{Participants}

We recruited 34 participants (17 pairs) from Amazon Mechanical Turk. All participants were from the U.S. Three pairs were excluded for making 2 or more errors on non-critical items.

\subsection{Materials \& Procedures}


Participants interacted in a real-time, multi-player environment on the web \cite{Hawkins15_RealTimeWebExperiments}. 
%To deal with participants arriving at unpredictable times, we implemented a waiting room mechanism, where the first person to enter the game environment would see a screen saying ``Waiting for another player..." and the second person would trigger the game to start. 
Pairs of participants---assigned randomly to `director' and `matcher' roles---interacted with one another through a web interface, shown in Fig. \ref{fig:interface}. On the left side of the screen, participants could freely type messages to one another; on the right side the screen, players could view a set of objects placed in a 4 x 4 grid. Five of the grid cells were occluded from only the director's perspective, and the remaining 11 were visible to both matcher and director. Six or seven objects were displayed in the grid at a given time. One of these objects was a `target', such as a cassette tape, placed in an unoccluded cell such that both participants could see it. Another object, such as a roll of tape, was placed in an occluded slot such that it was only visible to the matcher. The rest of the objects were unrelated `fillers' placed in random locations. We used the same set of targets and occluded alternatives as \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}, but we were unable to obtain the filler objects from the original experiment and created our own.
Before entering the game environment, every participant independently passed a short quiz about the task's instructions, ensuring that they understood the interface. Among other items on the quiz, we verified that both participants understood that items behind black cells were only visible to the matcher. 

The experiment was composed of eight items, with each item using a different set of objects. Each item included one `critical pair' of objects, one of them the target and the other hidden, such as the cassette tape and the roll of tape. For each item, we gave the director a series of four instructions to move objects around, which were displayed as a series of arrows pointing from some object to an unoccupied cell. 
To collect clean mouse-tracking data, we began every instruction by asking the matcher to click a small circle in the center of the grid. After this small circle was clicked, the director was allowed to communicate the next instruction and we started recording from the matcher's mouse. 
One of the instructions was a `critical instruction,' which referred to the target object.
For half the instructions, directors were free to communicate however they wished. For the other half, including all the critical instructions, their messages to the matcher were pre-scripted such that we could use the precise wording from \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}. For example, when giving instructions on how to move the cassette tape, the director would be forced to use the ambiguous utterance ``Move the tape down one square." (That is, in these conditions, the scripted message would automatically appear in the director's chat window, and they would have to click `send' for the experiment to continue.)

We collected baseline performance for each condition by replacing the hidden alternative (e.g.~a roll of tape) with an object that did not fit the critical instruction (e.g.~a battery); we used the same unambiguous replacements as \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}. Each participant received half the items in the experimental condition and half in the baseline condition. The assignment of items to conditions was randomized across participants, and the order of conditions was randomized under the constraint that the same condition would not be used on more than two consecutive items. All object sets, object placements, and corresponding instruction sets were the same for all participants.

This paradigm differs from those used by \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} in three primary ways. First, participants were not seated across from each other at a table: they each saw a views of the 4 x 4 grid on screen and communicated via a text box. Second, we did not use a trained confederate. We randomly assigned one of the players to the role of the instruction giver, and maintained the original wording by scripting a subset of their instructions. Finally, the hidden object was not placed in a bag, in which respect our design more closely resembles \citeA{KeysarBarr___Brauner00_TakingPerspective}.

%The experiment proceeded through all eight items, with the director providing instructions and the participant moving the objects. Except for instructions with scripted messages, the participants were free to use the chat box for questions, small talk, or whatever else.
\begin{table*}
\begin{center}
\begin{tabular}{ m{.75cm} | m{1.5cm} | m{1.5cm} | m{1.5cm} |  m{1.25cm} | m{1.6cm} | m{1.4cm} | m{1.3cm} | m{1.3cm} | m{1.5cm} |}
& & Item 1 & Item 2 & Item 3 & Item 4 & Item 5 & Item 6 & Item 7 & Item 8\\\hline
& instruction & ``glasses'' & ``bottom block'' & ``tape'' & ``large measuring cup'' & ``brush'' & ``eraser'' & ``small candle'' & ``mouse'' \\\hline
& target & sunglasses & block (3rd row) & cassette & medium cup & round hairbrush & board eraser & medium candle & computer mouse \\\hline
& hidden distractor & glasses case & block (4th row) & scotch-tape & large cup & flat \,\,\,hairbrush & pencil eraser & small candle & toy mouse \\
 \hline\hline
\multirow{2}{.75cm}{Exp.~1}  & \# incorrect & 0 & 5 & 1 & 3 & 2 &6 & 6 & 1 \\ 
                                               & \# correct & 6 & 3 & 1 & 8 & 2 & 2 & 4 & 6 \\
 \hline\hline
\multirow{2}{.75cm}{Exp.~2} & \# incorrect & 0 & 1 & 1 & 3 & 9 & 7 & 1 & 5 \\
                                              & \# correct   & 12&14&11&13&7 &8   &12& 8 \\

\end{tabular}
\caption{Item-wise error rates for critical trials}
\label{table:ItemWise}
\end{center}
\end{table*}


\subsection{Results and discussion}

In Table \ref{table:mainResults} we show the error rates on critical items, and compare to the data from
\citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}.
We find that 93\% of participants (all but one) attempted to move the hidden distractor at least once in the Experimental condition, out of four possible items, compared to only 7\% (only one) in the baseline condition. This is similar to the effect observed by the authors in the original study, which found 71\% and 0\%. Our errors were larger across the board, perhaps due to the interface or the population, but the gap between the two conditions is roughly the same size.
%Next, we examine whether there are differences in error rates across items. 
In Table \ref{table:ItemWise}, we break down the pattern of errors by item. 
%Because our randomization procedure undersampled the experimental condition for several items, a chi-squared test is inappropriate. Using Fisher's exact test, we cannot reject the null hypothesis of independence ($p = 0.13$). Still, 
We note that several conditions have much higher error rates than others -- for example, 75\% of participants in the experimental condition of item 6 made an error (the ``whiteboard eraser'' vs. the ``pencil eraser'') while only 17\% of participants in item 8 made an error (the ``computer mouse'' vs. the ``toy mouse''). 
This suggests that the dependent variable highlighted in the original study (i.e. ``percentage of participants who moved the critical item at least once'') is somewhat problematic. It could look like 100\% of participants made errors even if they all made those errors on one particularly difficult item. Indeed, if we exclude the three `hard' items where over 60\% of participants in the experimental condition made errors, this dependent variable drops from 93\% to only 43\% of participants. 
%This suggests that the ``at least one error'' DV is not appropriate in settings with high variability across items and that we might want to be more careful about controlling for this variability, perhaps by first measuring salience and ambiguity without the occlusion aspect of the paradigm.

As a proxy for the eye-tracking analyses reported by \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}, we conducted a mouse-tracking analysis. We define the decision window as the span of time between the point when the matcher received their instruction message and when they started moving an object. If it took them multiple attempts to move the correct object, we restricted our analysis to the first attempt. We took the set of all mouse position samples in the decision window (collected approximately every 15ms), and measured what proportion were located inside the cell containing the target vs. the cell containing the distractor.  We had to exclude an additional 3 participants for this analysis, because the timestamps for director and matcher did not align and we could not establish the decision window properly.
We found that there is an interaction between cell type and condition on the percent of hover time spent in that cell, $b = -0.21, t(11) = -3.1, p = 0.003$. In baseline trials, people spent much more time in the target cell and much less time in the distractor cell than in the experimental condition (see Fig. \ref{fig:exp1hover}).\footnote{Note that this analysis includes participants who actually made errors, since the data is too sparse to exclude them.} %accounts for a significant proportion of variance $F(3, 52) = 10.7, p < 0.001$

By running this replication as a multi-player web experiment we have available an additional source of data beyond the original experiments: half of the instructions were unscripted, providing observations of natural production of referential descriptions for filler items. 
Informally, we noted a tendency toward, possibly unnecessary, precision in descriptions. Instead of ``move the stuffed animal down'', participants say ``move the stuffed panda bear down.'' Or, instead of saying ``move the plane to the right'' when there is only one plane, participants say ``move the red airplane to the right.'' 
Perhaps directors were taking the time to make more precise descriptions because they believed it was contextually relevant: both parties know that there are hidden objects in the environment increasing the chance of miscommunication from imprecise descriptions. 
If the matcher expected the director to be precise, then they would be justified in picking the first or best object that meets the description (rather than worrying excessively about occluded cells). 
That is, the scripted instructions used by the director for critical trials may have been \emph{uncooperative}, and thus led matchers astray. 
We tested this prediction in Exp.~2, where we removed the scripted instructions and allowed speakers to refer to items however they wished.
By the reasoning above we expect to see more precise descriptions by unscripted directors and fewer errors by matchers in the critical trials.


%\subsection{Discussion}
%
%In both raw patterns of error and mouse-tracking metrics, we successfully replicated the findings of \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}. Participants frequently attempted to move the occluded alternative when it matched the target referential expression, even though the director couldn't possibly have been referring to it. Furthermore, the matcher tended to spend more time hovering their cursor over the occluded cell, and less time hovering over the target object, in ambiguous experimental trials than in baseline trials with no ambiguity. Both of these results support Keysar's claim that adults do not reliably deploy theory of mind reasoning when it is called for.
%
%Still, in running this replication as a multi-player web experiment, we observed two particularly interesting features in the data. First, 
%
%our analysis of item-level differences shows that the dependent variable highlighted in the original study (i.e. ``percentage of participants who moved the critical item at least once'') is somewhat problematic. It could look like 100\% of participants made errors even if they all made those errors on one particularly difficult item. Indeed, if we exclude the three `hard' items where over 60\% of participants in the experimental condition made errors, this dependent variable drops from 93\% to only 43\% of participants. This suggests that the ``at least one error'' DV is not appropriate in settings with high variability across items and that we might want to be more careful about controlling for this variability, perhaps by first measuring salience and ambiguity without the occlusion aspect of the paradigm.

%Second, because half of the instructions were unscripted, we were able to observe natural production of referential descriptions for a number of fillers. Informally, we noted a tendency toward over-informativity. Instead of ``move the stuffed animal down'', they'll say ``move the stuffed pa'nda bear down.'' Or, instead of saying ``move the plane to the right,'' they'll say ``move the red airplane to the right,'' even though there is only one plane. This suggested that the confederate's scripted instructions were in some sense uncooperative and counter to the matcher's expectations. Perhaps directors were taking the time to make more costly over-informative descriptions because they believed it was contextually relevant: both parties know that there are hidden objects in the environment. If the matcher reasoned in this way and expected the director to be overinformative, then they would be justified in picking the first object that meets the description rather than worrying about occluded cells. We tested this prediction in Exp.~2, where we removed the scripted instructions and allowed speakers to refer to items however they wished. If speakers really are being overinformative, and listeners are correctly expecting this, we should see fewer errors after removing uncooperative scripts.

\section{Exp.~2: Unscripted Replication}
\label{sec:Exp2}

\subsection{Participants}

We recruited 64 participants (32 pairs) from Amazon Mechanical Turk, roughly doubling the sample size from Exp.~1. All participants were from the U.S. Three participants were excluded for making 2 or more errors on non-critical items, and one additional participant was excluded because they were not a native English speaker.

\subsection{Materials \& Procedures}

Everything was the same as Exp.~1, except we did not use scripted messages for critical instructions. 
 
 \begin{figure}[b!]
\begin{center}
\includegraphics[scale=.75]{images/exp1MouseTracking.pdf}
\caption{Mean percentage of time spent hovering over the distractor and target cells in the experimental and baseline conditions. Error bars are 95\% confidence intervals.}
\label{fig:exp1hover}
\end{center}
\end{figure}

\begin{figure}[b!]
\begin{center}
\includegraphics[scale=.75]{images/exp2MouseTracking.pdf}
\caption{Mean percentage of time spent hovering over the distractor and target cells in the experimental and baseline conditions. Error bars are 95\% confidence intervals. \todo[inline]{TODO: Collapse mouse-tracking plots to a single figure showing some interaction}}
\label{fig:exp2hover}
\end{center}
\end{figure}

\subsection{Results}

Error rates are reported in Table \ref{table:mainResults}, alongside the results from \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} and our scripted replication in Exp.~1. Participants never moved the hidden object in the baseline condition, and total error rates for experimental trials are significantly lower than the rates found in Exp.~1, $\chi^2(1) = 5.35, p = 0.02$.

More dramatically, we find that patterns of errors in Exp.~2 diverge significantly from a uniform distribution across items, $\chi^2(7) = 24.8, p < 0.001$. Looking more closely at these patterns, we see that the only items where errors are consistently made are those where the costly over-informative utterance remains ambiguous. For example, in item 5, both the target and distractor are hair brushes: one is round and one is flat. ``Hair brush'' is already a label at the sub-class level. If we remove the two most difficult items (the ``hair brushes'' and the ``erasers''), the total percentage of errors on experimental trials drops from 24\% to 10\% and the percentage of participants making at least one error drops from 61\% to 32\%.

\todo[inline]{rdh: I don't like these mouse-tracking analyses -- I feel like the way I'm computing hover-time is sloppy and might be skewing the results (i.e. counting number of `move' events rather than actually looking at start and end times). So I could spend some time tomorrow coding up the better way\dots}
When we conducted a mouse-tracking analysis identical to the one reported for Exp.~1, we again found a significant interaction between condition and cell type on hover time, $b = -0.18, t(108) = -5.2, p < 0.001$. However, the effect appears to be attenuated: in experimental trials, participants spend significantly more time hovering over the target than the distractor (see Figure \ref{fig:exp2hover}) whereas in Exp.~1, participants spent roughly the same amount of time hovering over each. Due to the relatively small sample size of Exp.~1, however, we do not have the power to show this effect statistically.

Next, we test whether these improvements in performance are in fact due to different speaker behavior. We recruited twenty raters on Amazon Mechanical Turk, who provided ratings for how well the 71 unique labels used by speakers across both experiments (including scripted labels) fit the target and hidden distractor objects. Their responses were given on a slider with endpoints labeled ``not at all'' and ``perfectly.''  Inter-rater reliability was relatively high, with intra-class correlation coefficient of $0.6$ (95\% CI = $[0.54, 0.66]$).

In a mixed model including random intercepts for raters and items, we found a significant crossover interaction of experiment and referent on mean fitness rating, $b = 0.34, t = 36.4, p < 0.001$. In Exp.~1, the scripted label fit the hidden distractor better than the target, but the ordering reversed in Exp.~2: the unconstrained labels fit the target much better and the hidden distractor much worse (see Fig. \ref{fig:fitnessInteraction}).

Finally, we used a mixed effects logistic regression model to estimate the effect of distractor fit and speaker constraints on the probability of making an error on critical trials. We included random intercepts for game and item ids, and an interaction term for label fitness and label referent. We find a significant interaction of referent and fitness ratings on error rates: matchers are significantly more likely to make an error when the label fits the distractor more and the target less, $b =-1.39, z = -, p < 0.001$. Additionally, participants were significantly less likely to produce errors in Exp.~2 than Exp.~1, $b = -1.38, z = -2.41, p = 0.016$.

\todo[inline]{rdh: I still get confused about which random effects to include\dots does it make sense to include random intercepts for game id and include experiment \# as a predictor when each id only participated in one experiment?}

\begin{figure}[t!]
\begin{center}
\includegraphics[scale=.18]{images/fitnessInteraction.jpeg}
\caption{Mean fitness ratings provided by judges on Amazon Mechanical Turk. Error bars are bootstrapped 95\% confidence intervals.}
\label{fig:fitnessInteraction}
\end{center}
\end{figure}

%\begin{itemize}
%\item people still try to move occluded objects, but they do it less, and that behavior is more confined to a few particularly difficult items.
%\item this happens because the scripted instructions we forced directors to use in Exp.~1 were unnatural and violated listener expectations of overinformativity
%\item the fact that the effect is attenuated in exp. 2 is actually evidence of some theory of mind use (insofar as the director choosing to be overinformative, and the matcher expecting overinformativity constitutes ToM
%\end{itemize}

\section{General Discussion}

Pragmatic language understanding requires sophisticated social reasoning. To interpret an utterance, a listener must consider the speaker's communicative goals and context. In both experiments, we found evidence of such considerations: when the speaker's utterance fit the hidden distractor better than the target, matchers were more likely to make errors. This suggests that reference disambiguation was driven more by \emph{a priori} label fitness than consideration of the speaker's epistemic state, as Keysar and colleagues claim. 

However, this does not necessarily imply limitations on theory of mind. If listeners arrive at this interpretation strategy through pragmatic reasoning, then neglecting the speaker's epistemic state would be rational and also the result of theory of mind use. In Exp.~2, we found that such pragmatic interpretations were in fact justified: speakers naturally produce more precise, informative utterances than required and these unconstrained utterances fit the target significantly better than they fit the hidden distractor. Thus, the patterns of errors observed in Exp.~1 can be explained as an uncooperative confederate setting up certain pragmatic expectations through the task context, then deliberately flouting them in critical trials. % Removing constraints on the speaker produced a dramatic reduction in overall matcher errors and systematic variation in error rates across items. We found that unscripted speakers systematically tended to produce more precise, costly referring expressions than their scripted counterparts -- for example, ``the clear audiocassette'' when there was only one ``tape'' in the speaker's display. These precise labels were less likely to fit the hidden distractor, and therefore less likely to lead to a matcher error.

It is worth noting several significant differences between our study and \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} that prevented a direct replication. The primary difference between our study and the original, of course, is that it was run on the web with participants connected through a virtual environment, instead of face-to-face in a room. We believe we addressed the major concern about exploring theory of mind in web experiments -- that participants do not truly believe they are interacting with another human -- by allowing instantaneous, responsive, real-time interaction.  On the other hand, it is known that textual communication, as in our chat box, can differ from face-to-face verbal communication, and there was no way of addressing this. % Directors watched objects move through the actual moment-by-moment mouse trajectories that matchers used to drag them, and matchers saw the actual timing of the messages sent by the director.

A related difference is our decision not to use a confederate. While a confederate is useful for reducing variation across instances of the experiment, we see several downsides. Beyond the difficulties of conducting large-scale experiments with confederates on the web, it is difficult to exactly replicate all the subtleties of the confederate's behavior that might influence results. The pair of experiments we report is a reminder that manipulations administered by a trained confederate can interact in unexpected ways with a participant's social and communicative expectations. Regardless of the experimental context, it's illuminating to see how real participants naturally interact.

\todo[inline]{rdh: can take out next paragraph if hurting for space...}
A final concern is our graphical representation of occluded cells. We told players that items behind black cells were hidden to the director, showed them side-by-side displays of the director and matcher views, and quizzed them on this detail in the short attention check that players had to pass before playing. But they may have forgotten part-way through the experiment, or failed to fully internalize it. In the in-lab version, there was an actual divider between players, so it would have been more salient. We suspect that some subset of errors in our version game aren't due to a failure of perspective-taking, but instead because of a misunderstanding of what the other person's perspective is to begin with. However, this source of errors is held constant across our two experiments, and it would only bias our estimates of errors upward.

The Gricean maxim of quality dictates that cooperative speakers should make their utterance as informative as is required for current purposes, but no more so. If the referring expressions used in Exp.~1 uniquely pick out the target from the speaker's perspective, then why would unscripted speakers in Exp.~2 strive to be more informative than strictly necessary and why would listeners come to expect such ``overinformativity'' through social reasoning? Studies of overinformativity in referring expressions have uncovered a number of factors, such as the tendency to mention perceptually salient features to speed up the identification process  \cite{KoolenGattGoudbeekKrahmer11_Overspecification}. In Keysar's task, it seems particularly relevant that both participants knew about the presence of hidden objects, increasing uncertainty about possible distractors and placing additional pressure on the speaker to overspecify. 

It is interesting that the item-wise variability we observed in both experiments is at least partially due to the availability and effectiveness of more specific expressions. For example, no speaker in Exp.~2 produced ``the bottom block,'' which was used as the critical instruction in Exp.~1. Instead, they said ``the bluish block with a B'' or ``block with the blue writing,'' which relied on less confusable perceptual features and thereby decreased error rates. The items on which errors were most likely in Exp.~2 were difficult precisely because the overinformative utterance was \emph{still} ambiguous: even if the speaker uses ``hairbrush'' instead of the basic-level ``brush,'' for example, the distractor remains a more typical exemplar of the subcategory. 


\section{Acknowledgements}

\small We're grateful to Boaz Keysar for providing select materials for our replication. Exp.~1 was originally conducted under the supervision of Michael Frank, with early input from Desmond Ong. This work was supported by ONR grants N00014-13-1-0788 and N00014-13-1-0287,  and a James S. McDonnell Foundation Scholar Award to NDG. RXDH was supported by the Stanford Graduate Fellowship and the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747. 

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{bibs}


\end{document}
