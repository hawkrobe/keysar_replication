% arara: pdflatex
% arara: bibtex
% arara: pdflatex
% arara: pdflatex
% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.
 
\documentclass[10pt,letterpaper]{article}
 
\usepackage{hyperref}
\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{color}
\usepackage{todonotes}
\usepackage{dsfont}
\usepackage{textcomp}

\definecolor{Red}{RGB}{255,0,0}
\newcommand{\red}[1]{\textcolor{Red}{#1}}

\newcommand{\jd}[1]{\green{$^*$}\marginpar{\footnotesize{JD: \green{#1}}}}

\newcommand{\subsubsubsection}[1]{{\em #1}}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\tableref}[1]{Table \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\appref}[1]{Appendix \ref{#1}}
\newcommand{\sectionref}[1]{Section \ref{#1}}

\title{Pragmatics account for limits on theory of mind use in adults}
 
\author{{\large \bf Robert X.~D.~Hawkins, Noah D.~Goodman}\\
  \{rxdh,ngoodman\}@stanford.edu\\
  Department of Psychology, 450 Serra Mall \\
  Stanford, CA 94305 USA}


\begin{document}

\maketitle

\begin{abstract}
?

\textbf{Keywords:} 
?
\end{abstract}

\section{Introduction}
\label{sec:intro}

Humans can accurately and intelligently reason about the mental states of other humans. Among other things, this ability -- called \emph{theory of mind} \cite{PremackWoodruff78_ChimpanzeeToM} -- allows us to infer the underlying beliefs and intentions that motivate others' actions, and to use these inferences to predict future actions \cite{BakerSaxeTenenbaum09_ActionUnderstandingInversePlanning}. Most children acquire this ability by age six \cite{WimmerPerner83_BeliefsAboutBeliefs, WellmanCrossWatson01_ToMMetaAnalysis} and it serves as an important landmark in the developmental trajectory of intuitive theories \cite{GopnikWellman12_ReconstructingConstructivism}.

While theory of mind use often appears to be automatic and effortless, Keysar and colleagues \cite{KeysarBarr___Brauner00_TakingPerspective, KeysarLinBarr03_LimitsOnTheoryOfMindUse, LinKeysarEpley10_ReflexivelyMindblind} have argued that it is actually the opposite, even for adults: we are ``mindblind" by default and only overcome our egocentric biases through an effortful process of perspective-taking. In other words, while adults are \emph{capable} of applying theory of mind reasoning, we do not always apply it reliably. This argument is based on a simple experimental paradigm, where participants played a simple communication game with a confederate. The two players were placed on opposite sides of a $4 \times 4$ grid containing a set of everyday objects. The confederate played the role of `director,' giving instructions about how to move objects around a grid, and the participant played the role of `matcher,' attempting to follow these instructions. For example, the objects in one trial included a medium-sized measuring cup in one location and a smaller measuring cup in another location. The director gave an instruction like `move the large cup up one square,' referring to the medium-sized one. 

Some objects were occluded such that only the matcher could see them, creating an asymmetry in the players' knowledge. To perform accurately, the matcher would need to apply theory of mind and reason about the director?s beliefs. For example, if a much larger measuring cup were placed in an occluded slot and agents failed to account for the director?s beliefs, they might interpret `the large cup' to mean this occluded cup even though the director couldn't possibly know about it. Indeed, \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} found that participants attempted to move the hidden item in 30\% of cases: 71\% of participants attempted to move this hidden item at least once (out of four critical cases) in the experimental condition, compared to 0\% in a control condition where there was no ambiguity over the referent. 46\% of participants reached for the hidden object at least twice. Additionally, eye-tracking data showed that participants considered the hidden item more often and for longer in the experimental condition than the control condition. 

While these results are compelling, the paradigm has been criticized from several different angles. First, in all experiments, the privileged object was a better fit for the referring expression than the one in common ground (e.g. the largest measuring cup vs. the medium measuring cup for ``the large measuring cup"), making the privileged object \emph{a priori} more likely to be the referent. It would be fairer to compare two objects that fit the referring expression equally well \cite{HellerGrodnerTanenhaus08_Perspective}. Second, the viewpoint asymmetry paradigm is somewhat unnatural: common ground is typically built incrementally over the course of an interaction rather than presented all at once, and it is rare for a shared display to differ in perceptual accessibility \cite{HannaTanenhausTrueswell03_CommonGroundPerspective}. Third, none of the experiments reported by Keysar and colleagues included a key comparison condition where the critical item (e.g. the largest measuring cup) was \emph{also} in common ground \cite{BrownSchmidtHanna11_IncrementalPerspectiveTaking}. It would be useful to know whether people referred to the critical item less in the privileged condition than in the full common ground condition, which would put a more positive spin on the results. %Note that none of these criticisms invalidate the paradoxical result that listeners move an occluded object, they just suggest an alternative explanation. Reference disambiguation is probabilistic: common ground and goodness of fit are weighed against one another.

In this paper, we offer a novel pragmatic account of Keysar's results. When both players know that objects are occluded in the display, we note that the speaker is likely to be overinformative. If the listener \emph{expects} the speaker to be overinformative, they will pragmatically pick the \emph{a priori} more likely referent of the referring expression, which in critical trials will be the occluded object. In other words, it is precisely \emph{because} the listener takes the speaker's mental state into consideration that they are tricked by an uncooperative confederate into choosing the wrong item. We began by replicating \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} (in a multi-player web experiment, recruiting participants to be both director and matcher instead of using a confederate. Instructions for critical items, as well as a random subset of filler items, remained scripted as in the original study. We then ran the same experiment without scripted items, observing unconstrained director behavior. This minimal pair of experiments demonstrates that listener mistakes are at least partially due to the pragmatics of the task, ironically showing that apparent failures of theory of mind are in fact attributable to theory of mind.

\begin{figure*}
\begin{center}
\includegraphics[scale = .35]{images/overall_view.jpg}
\end{center}
\vspace{-.25cm}
\caption{Interface used in the reported experiments. Objects behind the black squares were hidden from the director.}
\label{fig:interface}
\end{figure*}

\section{Exp.~1: Scripted Replication}
\label{sec:Exp1}

\subsection{Participants:}

We recruited 34 participants (17 pairs) from Amazon Mechanical Turk. All participants were from the U.S. Three pairs were excluded for making 2 or more errors on non-critical items.

\subsection{Materials:}

Pairs of participants -- assigned randomly to `director' and `matcher' roles -- interacted with one another through a real-time web interface, shown in Fig. \ref{fig:interface}. On the left side of the screen, participants could freely type messages to one another; on the right side the screen, players could view a set of objects placed in a 4 x 4 grid. Five of the grid cells were occluded from the director's perspective, and the remaining 11 were visible to both participant and director. Six or seven objects were displayed in the grid at a given time. One of these objects was a `target', such as a cassette tape, placed in an unoccluded cell such that both participants could see it. Another object, such as a roll of tape, was placed in an occluded slot such that it was only visible to the matcher. The rest of the objects were unrelated `fillers' placed in random locations. We used the same set of targets and occluded alternatives as \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}, but we were unable to obtain the filler objects from the original experiment and created our own.

The experiment was composed of eight items, with each item using a different set of objects. Each item included one `critical pair' of objects, one of them the target and the other hidden, such as the cassette tape and the roll of tape. For each item, we gave the director a series of four instructions to move objects around, which were displayed as a series of arrows pointing from some object to an unoccupied cell. One of these instructions was a `critical instruction,' which referred to the target object. For half the instructions, directors were free to communicate however they wished. For the other half, including the critical instruction, their messages to the matcher were pre-scripted such that we could use the precise wording from \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}. For example, when giving instructions on how to move the cassette tape, the director would be forced to use the ambiguous utterance ``Move the tape down one square." 

We collected baseline performance for each condition by replacing the hidden alternative (e.g. a roll of tape) with an object that did not fit the critical instruction (e.g. a battery). We also used the same unambiguous replacements as \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}. Each participant received half the items in the experimental condition and half in the baseline condition. The assignment of items to conditions was randomized across participants, and the order of conditions was randomized under the constraint that the same condition would not be used on more than two consecutive items. All object sets, object placements, and corresponding instruction sets were the same for all participants.

These materials differ from those used by \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} in a few ways. First, participants were not seated across from each other at a table: they each saw a views of the 4 x 4 grid on screen and communicated via a text box. Second, we did not use a trained confederate. We randomly assigned one of the players to the role of the instruction giver, and maintained the original wording by scripting a subset of their instructions. Finally, the hidden object was not placed in a bag, in which respect our design more closely resembles \citeA{KeysarBarr___Brauner00_TakingPerspective}.
\begin{table*}
\begin{center}
\begin{tabular}{ p{2cm} | r | r |  r | r | r | r}
& \multicolumn{2}{c|}{\% attempted at least once} & \multicolumn{2}{c|}{\% attempted at least twice} & \multicolumn{2}{c}{\% of total cases}\\
\hline
& Original & Replication & Original & Replication & Original & Replication \\
Experimental & 71 & 93 & 46 & 64 & 30 & 37 \\
Baseline & 0 & 14 & 0 & 0 & 0 & 3 \\
\end{tabular}
\caption{Side-by-side comparison of error rates in \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} and our replication. The first column shows the percentage of \emph{participants} attempting to move the occluded distractor at least one of the four possible cases. The second column shows the percentage attempting to move it at least twice. The third column shows the percentage of \emph{all experimental trials} that the participant actually tried to move the occluded object. \todo[inline]{Big caption maybe unnecessary}}
\label{table:exp1}
\end{center}
\end{table*}


\subsection{Procedure}

Participants interacted in a real-time, multi-player environment on the web \cite{Hawkins15_RealTimeWebExperiments}. Before entering this environment, every participant independently passed a short quiz about the task's instructions, ensuring that they understood the interface. Among other items on the quiz, we verified that both participants understood that items behind black cells were only visible to the matcher. To deal with participants arriving at unpredictable times, we implemented a waiting room mechanism, where the first person to enter the game environment would see a screen saying ``Waiting for another player..." and the second person would trigger the game to start. 

To collect clean mouse-tracking data, we began every instruction by asking the matcher to click a small circle in the center of the grid. After this small circle was clicked, the director was allowed to communicate the next instruction and we started recording from the matcher's mouse. The experiment proceeded through all eight items, with the director providing instructions and the participant moving the objects. Except for instructions with scripted messages, the participants were free to use the chat box for questions, small talk, or whatever else.

\subsection{Results}

Using the error rates on critical items, we compare our behavioral results with those from from \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} in Table \ref{table:exp1}. We find that 93\% of participants attempted to move a hidden object at least once in the Experiment condition, out of four possible items, compared to only 14\% in the baseline condition. This is similar to the effect observed by the authors in the original study, which found 71\% and 0\%. Our errors were larger across the board, perhaps due to the interface and the Turk population, but the gap between the two is roughly the same size.

Next, we examine whether there are differences in error rates across items. In Table \ref{table:itemWiseExp1}, we break down the pattern of errors by item. Because our randomization procedure undersampled the experimental condition for several items, a chi-squared test is inappropriate. Using Fisher's exact test, we cannot reject the null hypothesis of independence ($p = 0.13$). Still, we note that several conditions have much higher error rates than others -- for example, 75\% of participants in the experimental condition of item 6 made an error (the ``whiteboard eraser'' vs. the ``pencil eraser'') while only 17\% of participants in item 8 made an error (the ``computer mouse'' vs. the ``toy mouse''). 

Finally, as a proxy for the eye-tracking analyses reported by \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse}, we conducted a mouse-tracking analysis. We define the decision window as the span of time between the point when the matcher received their instruction message and when they started moving an object. If it took them multiple attempts to move the correct object, we restricted our analysis to the first attempt. We took the set of all mouse position samples in the decision window (collected approximately every 15ms), and measured what proportion were located inside the cell containing the target vs. the cell containing the distractor.  We had to exclude an additional 3 participants for this analysis, because the timestamps for director and matcher did not align and we could not establish the decision window properly.

We found that there is an interaction between cell type and condition on the percent of hover time spent in that cell, $b = -0.21, t(11) = -3.1, p = 0.003$. In baseline trials, people spent much more time in the target cell and much less time in the distractor cell than in the experimental condition (see Fig. \ref{fig:exp1hover}). Note that this analysis includes participants who actually made errors, since the data is too sparse to exclude them. %accounts for a significant proportion of variance $F(3, 52) = 10.7, p < 0.001$

\begin{table*}
\begin{center}
\begin{tabular}{ p{2cm} | r | r |  r | r | r | r | r | r |}
& Item 1 & Item 2 & Item 3 & Item 4 & Item 5 & Item 6 & Item 7 & Item 8\\\hline
\# incorrect & 1 & 5 & 1 & 3 & 2 &6 & 6 & 1 \\ 
\# correct & 5 & 3 & 1 & 8 & 2 & 2 & 4 & 6 \\
\end{tabular}
\caption{Item-wise error analysis}
\label{table:exp1}
\end{center}
\end{table*}

\begin{figure}[b!]
\begin{center}
\includegraphics[scale=.8]{images/exp1MouseTracking.pdf}
\caption{Mean percentage of time spent hovering over the distractor and target cells in the experimental and baseline conditions. Error bars are 95\% confidence intervals.}
\label{fig:exp1hover}
\end{center}
\end{figure}

\subsection{Discussion}

\begin{itemize}
\item quibble with ?moved at least one thing? measure (allows one difficult item to dominate if item-wise differences)
\item in non-scripted instructions, noticed that people were being overinformative
\end{itemize}
        
We noticed several difference between our study and \citeA{KeysarLinBarr03_LimitsOnTheoryOfMindUse} that prevented a direct replication. The primary difference between our study and the original is the fact that it will be run online with participants connected via a virtual environment, instead of face-to-face in a room. A related difference is our decision not to use a confederate. In general, studies using confederates are difficult to replicate exactly and are impractical to run online. We are aware that this change (while keeping scripted instructions, which were the primary justification for the confederate) may lead to additional variation across groups. It is also known that textual communication (via our instant messaging interface) can differ from verbal communication (as in the original study). Finally, the way people attempt to move things in our virtual environment may be different from how people attempt to move things in the lab, and we cannot eye-tracking data to back up our analysis, as the original paper did.


\section{Exp.~2: Unscripted Replication}
\label{sec:Exp2}

\subsection{Methods}

 same as exp 1, except no scripted instructions (i.e. we don?t force the director to say ?tape?)
 
\subsection{Results}

\begin{itemize}
\item Listener makes fewer mistakes?
\item Director more overinformative on critical items?
\end{itemize}

\subsection{Discussion}

\begin{itemize}
\item scripted instruction unnatural, 
\item violates listener expectations of overinformativity
\end{itemize}

\section{General Discussion}

First, our analysis of item-level differences shows that the dependent variable used in the original study (i.e. ?percentage of participants who moved the critical item at least once?) is somewhat problematic. It could look like 100% of participants made the errors even if they all made those errors on one particularly difficult item (e.g. the ?bottom? block, where they actually mean the one on the second-to-bottom row). If we exclude the three ?hard? items (2, 6, and 7) where over 60% of participants in the experimental condition made errors, our results look much less strong than Keysar?s. 



At least once
At least twice
Experiment
43
14
Baseline
7
0

This suggests that (1) the ?at least one error? DV is not appropriate in settings with high variability across items and (2) we might want to be more careful about controlling for this variability, perhaps by first measuring salience and ambiguity without the occlusion aspect of the paradigm.

Second, it?s worth thinking about some ways our online version may differ from the original. The fact that we didn?t use a confederate didn?t seem to be a problem, given our use of randomly scripted instructions. Similarly, because players could type messages directly to one another, and since the director could watch the matcher moving objects in real-time, it?s also fair to say that participants truly believed they were playing with another human being. 

One major concern is our graphical representation of occluded cells. We simply told players that items behind black cells were hidden to the director, and this was a critical question in the quiz that players had to pass before playing, so we know that they were aware of it. But they may forget part-way through the experiment, or don?t fully internalize it. In the in-lab version, there was an actual divider between players, so it would have been more salient. This would mean that some errors in our version game aren?t due to a failure of perspective-taking, but instead because of a misunderstanding of what the other person?s perspective is to begin with. However, this makes our task 

Running multi-player control w/o confederate is important to know whether confederate behavior is valid and cooperative

\section{Acknowledgements}

\small We're grateful to Boaz Keysar for providing select materials for our replication. Exp.~1 was originally conducted under the supervision of Michael Frank, with early input from Desmond Ong. This work was supported by ONR grants N00014-13-1-0788 and N00014-13-1-0287,  and a James S. McDonnell Foundation Scholar Award to NDG. RXDH was supported by the Stanford Graduate Fellowship and the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747. 

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{bibs}


\end{document}
