// webppl noisyContextRSA.wppl --require ./refModule/ -- --model occlusionSensitivity
// webppl noisyContextRSA.wppl --require ./refModule/ -- --model egocentric
// webppl noisyContextRSA.wppl --require ./refModule/ -- --model egocentric --AIS true

var data = refModule.readCSV('./input/bdaInput.csv');

var possibleObjects = Categorical({
  vs: [
    'texture_color_shape', 'othertexture_othercolor_othershape',
    'othertexture_color_shape', 'texture_othercolor_shape', 'texture_color_othershape',
    'othertexture_othercolor_shape', 'texture_othercolor_othershape', 'othertexture_color_othershape',
  ]
  //ps: [1/64, 27/64, 3/64, 3/64, 3/64, 9/64, 9/64, 9/64]
});

// speaker can mention any combination of the 3 feature dimensions
var possibleUtterances = Categorical({
  vs: ['shape', 'color', 'texture',
       'texture_color', 'color_shape', 'texture_shape',
       'texture_color_shape',
       'othershape', 'othercolor', 'othertexture',
       'othertexture_othercolor', 'othercolor_othershape', 'othertexture_othershape',
       'othertexture_othercolor_othershape',
       'othertexture_color', 'texture_othercolor', 'color_othershape', 'othercolor_shape',
       'othertexture_shape', 'texture_othershape', 'texture_color_othershape', 'texture_othercolor_shape',
       'othertexture_color_shape', 'texture_othercolor_othershape', 'othertexture_color_othershape',
       'othertexture_othercolor_shape']
});

// these are the possible mixture weights for perspective-taking
// where 0 is purely egocentric and 1 is purely partner's perspective
var possibleWeightings = Categorical({
  vs: _.range(0, 1, 0.05)
});

var contexts = {
  'diff_shape' : ['texture_color_shape', 'othertexture_othercolor_othershape'],
  'shape_only' : ['texture_color_shape', 'othertexture_othercolor_shape'],
  'color_shape' : ['texture_color_shape', 'othertexture_color_shape'],
  'texture_shape' : ['texture_color_shape', 'texture_othercolor_shape']
};

var getTrueUtterances = function(context) {
  var trueUtts = filter(function(utt) {
    return _.some(map(function(obj) {
      return meaning(utt, obj);
    }, context));
  }, possibleUtterances.support());
  return Categorical({vs: trueUtts});
};

// utterance is true if object has all of the mentioned features
var meaning = function(utt, object) {
  var descriptors = utt.split('_');
  var objProperties = object.split('_').slice(0,3);
  var matches = _.every(map(function(descriptor) {
    return _.includes(objProperties, descriptor);
  }, descriptors));
  return matches;
};

// cost of producing utterance is sum of costs of the component words
var uttCost = function(utt, params) {
  var descriptors = utt.split('_');
  return sum(map(function(descriptor) {
    return descriptor == 'null' ? 0 : params[descriptor.replace('other', '') + 'Cost'];
  }, descriptors));
};

// derives the speaker's perspective from the listener's visible context
var getSpeakerView = function(listenerContext) {
  var hiddenObj = find(function(x) {return last(x.split('_')) == 'private';}, listenerContext);
  return remove(hiddenObj, listenerContext);
};


var L0 = cache(function(utt, context, weighting) {
  return Infer({method: 'enumerate'}, function() {
    var perspective = flip(weighting) ? 'other' : 'own';
    var partnerContext = perspective == 'own' ? context : getSpeakerView(context);
    var object = uniformDraw(partnerContext);
    condition(meaning(utt, object));
    return object;
  });
});

// S1 selects utterance using fixed mixture of perspectives
// (given belief about L0's mixture)
var S1 = cache(function(target, context, ownWeighting, params) {
  return Infer({method: 'enumerate'}, function() {
    var utt = sample(getTrueUtterances(context));    
    var partnerWeighting = sample(possibleWeightings);

    // Weight proportional to utility
    var perspective = flip(ownWeighting) ? 'other' : 'own';
    
    var informativity = (
      perspective == 'own' ? L0(utt, context, 0).score(target) :
        expectation(possibleObjects, function(obj) {
          var possibleListenerView = params.hidden =='yes' ? context.concat(obj + '_private') : context;
          return (L0(utt, possibleListenerView, partnerWeighting).score(target));
        })
    );

    factor(params.alpha * (informativity - uttCost(utt,params)));
    return utt;
  });
});

var getResponse = cache(function(trialInfo) {
  var responses = map(function(attribute) {
    return trialInfo[attribute] == 'TRUE' ? attribute.replace('Mention', '') : '';
  }, ['textureMention', 'colorMention', 'shapeMention']);
  return filter(function(v) {return v != '';}, responses).join('_');
});

var getExpectedLength = function(condition, additionalDistractors, params) {
  return expectation(Infer({method: 'enumerate'}, function() {
    var additionalDs = repeat(additionalDistractors, function() {
      return flip() ? 'texture_othercolor_othershape' : 'othertexture_color_othershape'
    });
    var knownContext = contexts[condition.distractorType].concat(additionalDs);
    var dist = S1('texture_color_shape', knownContext, params.ownWeighting,
                  extend({}, params, {hidden: condition.hidden}));
    return sample(dist).split('_').length;
  }));
};		    

var bda = function() {
  var params = {
    alpha : uniformDrift({a: 0, b: 1000, width: 30}),
    ownWeighting: (argv.model == 'egocentric' ? 0 :
                   argv.model == 'occlusionSensitive' ? 1 :
                   argv.model == 'mixture' ? uniformDrift({a: 0, b: 1, width: 0.1}) :
                   console.error('unknown choice of model:', argv.model)),
    textureCost: Math.exp(uniformDrift({a: -10, b: 1, width: 1})),
    colorCost: Math.exp(uniformDrift({a: -10, b: 1, width: 1})),
    shapeCost: Math.exp(uniformDrift({a: -10, b: 1, width: 1}))
  };

  globalStore.score = 0;
  mapData({data: data}, function(datum) {
    var knownContext = contexts[datum.distractorType];
    var modelPrediction = S1('texture_color_shape', knownContext, params.ownWeighting,
  			     _.extend({}, params, {hidden: datum.hidden}));
    var response = getResponse(datum);
    var mixtureDist = Mixture({dists: [modelPrediction, possibleUtterances], ps: [0.95, 0.05]});
    var score = mixtureDist.score(response);
    factor(score);
    globalStore.score += score;
  });
  return {
    params: _.zipObject([_.values(params).join(',')], [globalStore.score]),
    objParams: params
  };
};

if(argv.AIS == "true"){
  console.log([argv.model, AIS(bda, {samples: 1, steps: 5000})].join(','));
} else {
  var outputERP = Infer({method: 'MCMC', samples: 1000, burn: 1000, lag: 1, model: bda, verbose: true});
  refModule.paramsErpWriter(outputERP, "./output/" + argv.model + "BDA");

  var predictives = function() {
    var params = sample(outputERP)['objParams'];
    console.log(params);
    return {
      predictives: _.zipObject([[
        getExpectedLength({'distractorType': 'texture_shape', 'hidden' : 'yes'}, 0, params),
        getExpectedLength({'distractorType': 'texture_shape', 'hidden' : 'yes'}, 1, params),
        getExpectedLength({'distractorType': 'texture_shape', 'hidden' : 'yes'}, 2, params), 
        getExpectedLength({'distractorType': 'color_shape', 'hidden' : 'yes'}, 0, params),
        getExpectedLength({'distractorType': 'color_shape', 'hidden' : 'yes'}, 1, params),
        getExpectedLength({'distractorType': 'color_shape', 'hidden' : 'yes'}, 2, params),
        getExpectedLength({'distractorType': 'shape_only', 'hidden' : 'yes'}, 0, params),
        getExpectedLength({'distractorType': 'shape_only', 'hidden' : 'yes'}, 1, params),
        getExpectedLength({'distractorType': 'shape_only', 'hidden' : 'yes'}, 2, params),
        getExpectedLength({'distractorType': 'diff_shape', 'hidden' : 'yes'}, 0, params),
        getExpectedLength({'distractorType': 'diff_shape', 'hidden' : 'yes'}, 1, params),
        getExpectedLength({'distractorType': 'diff_shape', 'hidden' : 'yes'}, 2, params),      
        getExpectedLength({'distractorType': 'texture_shape', 'hidden' : 'no'}, 0, params),
        getExpectedLength({'distractorType': 'texture_shape', 'hidden' : 'no'}, 1, params),
        getExpectedLength({'distractorType': 'texture_shape', 'hidden' : 'no'}, 2, params),
        getExpectedLength({'distractorType': 'color_shape', 'hidden' : 'no'}, 0, params),
        getExpectedLength({'distractorType': 'color_shape', 'hidden' : 'no'}, 1, params),
        getExpectedLength({'distractorType': 'color_shape', 'hidden' : 'no'}, 2, params),
        getExpectedLength({'distractorType': 'shape_only', 'hidden' : 'no'}, 0, params),
        getExpectedLength({'distractorType': 'shape_only', 'hidden' : 'no'}, 1, params),
        getExpectedLength({'distractorType': 'shape_only', 'hidden' : 'no'}, 2, params),
        getExpectedLength({'distractorType': 'diff_shape', 'hidden' : 'no'}, 0, params),
        getExpectedLength({'distractorType': 'diff_shape', 'hidden' : 'no'}, 1, params),
        getExpectedLength({'distractorType': 'diff_shape', 'hidden' : 'no'}, 2, params)      
      ].join(',')])
    };
  };

  var predictiveOutput = Infer({method: 'forward', samples: 1000, model: predictives, verbose: true});
  console.log(predictiveOutput.samples);
  refModule.predictivesErpWriter(predictiveOutput, "./output/" + argv.model + "BDA");
}
