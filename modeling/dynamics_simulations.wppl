// webppl experimental.wppl

// the stimulus set in this world varies along 3 dimensions. because
// the precise levels (e.g. 'blue', 'red') don't matter, we always
// take the target to be 'texture_color_shape' (i.e. some set of
// levels) and allow distractors to differ along any of these levels
// (e.g. 'otherTexture_otherColor_shape' has the same shape as the
// target, but different texture and color). the (optional)
// probabilities the true probabilities of hidden distractors in the
// experiment, if objects are sampled uniformly from the 4 x 4 x 4
// space.
var possibleObjects = Categorical({
  vs: [
    'texture_color_shape', 'othertexture_othercolor_othershape',
    'othertexture_color_shape', 'texture_othercolor_shape', 'texture_color_othershape',
    'othertexture_othercolor_shape', 'texture_othercolor_othershape', 'othertexture_color_othershape',
  ]
//  ps: [1/64, 27/64, 3/64, 3/64, 3/64, 9/64, 9/64, 9/64]
});

// speaker can mention any combination of the 3 feature dimensions
var possibleUtterances = Categorical({
  vs: ['shape', 'color', 'texture',
       'texture_color', 'color_shape', 'texture_shape',
       'texture_color_shape',
       'othershape', 'othercolor', 'othertexture',
       'othertexture_othercolor', 'othercolor_othershape', 'othertexture_othershape',
       'othertexture_othercolor_othershape',
       'othertexture_color', 'texture_othercolor', 'color_othershape', 'othercolor_shape',
       'othertexture_shape', 'texture_othershape', 'texture_color_othershape', 'texture_othercolor_shape',
       'othertexture_color_shape', 'texture_othercolor_othershape', 'othertexture_color_othershape',
       'othertexture_othercolor_shape']
});

// these are the possible mixture weights for perspective-taking
// where 0 is purely egocentric and 1 is purely partner's perspective
var possibleWeightings = Categorical({
  vs: _.range(0, 1.1, 0.1)
});

// these are the possible conditions of visible objects presented to the speaker
var contexts = {
  'diff_shape' : ['texture_color_shape', 'othertexture_othercolor_othershape'],
  'shape_only' : ['texture_color_shape', 'othertexture_othercolor_shape'],
  'color_shape' : ['texture_color_shape', 'othertexture_color_shape'],
  'texture_shape' : ['texture_color_shape', 'texture_othercolor_shape']
};

// alpha: soft-max temperature
// <feature>Cost: cost associated with producing this feature
// perspectiveCost: cost associated with perspective-taking
var params = {
  alpha: 5,
  textureCost : 0.01,
  shapeCost : 0.01,
  colorCost : 0.01,
  perspectiveCost : 0.1
};

var getTrueUtterances = function(context) {
  var trueUtts = filter(function(utt) {
    return _.some(map(function(obj) {
      return meaning(utt, obj);
    }, context));
  }, possibleUtterances.support());
  return Categorical({vs: trueUtts});
};

// utterance is true if object has all of the mentioned features
var meaning = function(utt, object) {
  var descriptors = utt.split('_');
  var objProperties = object.split('_').slice(0,3);
  var matches = _.every(map(function(descriptor) {
    return _.includes(objProperties, descriptor);
  }, descriptors));
  return matches;
};

// cost of producing utterance is sum of costs of the component words
var uttCost = function(utt) {
  var descriptors = utt.split('_');
  return sum(map(function(descriptor) {
    return descriptor == 'null' ? 0 : params[descriptor.replace('other', '') + 'Cost'];
  }, descriptors));
};

// derives the speaker's perspective from the listener's visible context
var getSpeakerView = function(listenerContext) {
  var hiddenObj = find(function(x) {return last(x.split('_')) == 'private';}, listenerContext);
  return remove(hiddenObj, listenerContext);
};

// L0 interprets utterance literally using fixed mixture of perspectives
var L0 = cache(function(utt, context, weighting) {
  return Infer({method: 'enumerate'}, function() {
    var perspective = flip(weighting) ? 'other' : 'own';
    var partnerContext = perspective == 'own' ? context : getSpeakerView(context);
    var object = uniformDraw(partnerContext);
    condition(meaning(utt, object));
    return object;
  });
});

// S1 selects utterance using fixed mixture of perspectives
// (given belief about L0's mixture)
var S1 = cache(function(target, context, ownWeighting) {
  return Infer({method: 'enumerate'}, function() {
    var utt = sample(getTrueUtterances(context));    
    var partnerWeighting = sample(possibleWeightings);

    // Weight proportional to utility
    var perspective = flip(ownWeighting) ? 'other' : 'own';
    
    var informativity = (
      perspective == 'own' ? L0(utt, context, 0).score(target) :
        expectation(possibleObjects, function(obj) {
          var possibleListenerView = context.concat(obj + '_private');
          return (L0(utt, possibleListenerView, partnerWeighting).score(target));
        })
    );

    factor(params.alpha * (informativity - uttCost(utt,params)));
    return utt;
  });
});

// L1 selects objects given belief about S1's mixture
var L1 = function(utt, context, ownWeighting) {
  return Infer({method: 'enumerate'}, function() {
    var perspective = flip(ownWeighting) ? 'other' : 'own';
    var partnerContext = perspective == 'own' ? context : getSpeakerView(context);
    var partnerWeight = perspective == 'own' ? 0 : sample(possibleWeightings);

    var object = uniformDraw(partnerContext);
    observe(S1(object, partnerContext, partnerWeight), utt);
    return object;
  });
};


var listenerPosterior = function(data) {
  return Infer({method: 'enumerate'}, function() {
    var partnerWeighting = sample(possibleWeightings);
    mapData({data: data}, function(datum) {
      observe(S1(datum.target, datum.context, partnerWeighting), datum.utt);
    });
    return partnerWeighting;
  });
};

var posterior = listenerPosterior([
  {utt: 'texture', target: 'texture_color_shape', context: contexts['diff_shape']},
  {utt: 'texture', target: 'texture_color_shape', context: contexts['diff_shape']},
  {utt: 'texture', target: 'texture_color_shape', context: contexts['diff_shape']},
  {utt: 'texture', target: 'texture_color_shape', context: contexts['diff_shape']}
])

// meta-cognitive resource-rational speaker selects optimal mixture
// weight, marginalizing over uncertainty about listener's weight
var RR_speaker = function(target, context) {
  return Infer({method: 'enumerate'}, function() {
    var ownWeighting = sample(possibleWeightings);

    // Imagine how speaker with this weight would behave
    var likelyUtt = MAP(S1(target, context, ownWeighting))['val'];
    
    // Imagine expected utility of that behavior, marginalizing over
    // partner's mixture weight and possible objects in partner's view
    var utility = expectation(possibleObjects, function(obj) {
      return expectation(possibleWeightings, function(partnerWeighting) {
        var possibleListenerView = context.concat(obj + '_private');
        return L0(likelyUtt, possibleListenerView, partnerWeighting).score(target);
      });
    });

    // putting more weight on partner's perspective is costly
    factor(utility - ownWeighting * params.perspectiveCost);
    return ownWeighting;
  });
}

// meta-cognitive resource-rational speaker selects optimal mixture
// weight, marginalizing over uncertainty about listener's weight
var RR_listener = function(utt, baseContext) {
  return Infer({method: 'enumerate'}, function() {
    var ownWeighting = sample(possibleWeightings);

    // Marginalize over partner's mixture weight and possible hidden objects
    var utility = expectation(possibleObjects, function(hiddenObj) {
      return expectation(posterior, function(partnerWeighting) {
        var context = baseContext.concat(hiddenObj + '_private');
        var realSpeakerView = getSpeakerView(context);
        var likelyUtt = MAP(S1('texture_color_shape', realSpeakerView, partnerWeighting))['val'];
        return L1(likelyUtt, context, ownWeighting).score('texture_color_shape');
      });
    });

    // putting more weight on partner's perspective is costly
    factor(utility - ownWeighting * params.perspectiveCost);
    return ownWeighting;
  });
};


console.log(MAP(RR_listener('texture_color_shape', contexts['diff_shape'])));
