// webppl experimental.wppl

// the stimulus set in this world varies along 3 dimensions. because
// the precise levels (e.g. 'blue', 'red') don't matter, we always
// take the target to be 'texture_color_shape' (i.e. some set of
// levels) and allow distractors to differ along any of these levels
// (e.g. 'otherTexture_otherColor_shape' has the same shape as the
// target, but different texture and color). the (optional)
// probabilities the true probabilities of hidden distractors in the
// experiment, if objects are sampled uniformly from the 4 x 4 x 4
// space.
var possibleObjects = Categorical({
  vs: ['texture_color_shape', 'otherTexture_otherColor_otherShape',
       'otherTexture_color_shape', 'texture_otherColor_shape', 'texture_color_otherShape',
       'otherTexture_otherColor_shape', 'texture_otherColor_otherShape', 'otherTexture_color_otherShape',
      ]
//  ps: [1/64, 3/64, 3/64, 9/64, 3/64, 9/64, 9/64, 27/64]
});

// speaker can mention any combination of the 3 feature dimensions
var possibleUtterances = Categorical({
  vs: ['shape', 'color', 'texture',
       'texture_color', 'color_shape', 'texture_shape',
       'texture_color_shape']
});

// these are the possible mixture weights for perspective-taking
// where 0 is purely egocentric and 1 is purely partner's perspective
var possibleWeightings = Categorical({
  vs: _.range(0, 1.1, 0.1)
});

// these are the possible conditions of visible objects presented to the speaker
var contexts = {
  'diff_shape' : ['texture_color_shape', 'otherTexture_otherColor_otherShape'],
  'shape_only' : ['texture_color_shape', 'otherTexture_otherColor_shape'],
  'color_shape' : ['texture_color_shape', 'otherTexture_color_shape'],
  'texture_shape' : ['texture_color_shape', 'texture_otherColor_shape']
};

// alpha: soft-max temperature
// <feature>Cost: cost associated with producing this feature
// perspectiveCost: cost associated with perspective-taking
var params = {
  alpha: 10,
  textureCost : 0.01,
  shapeCost : 0.01,
  colorCost : 0.01,
  perspectiveCost : 0.1
};

// utterance is true if object has all of the mentioned features
var meaning = function(utt, object) {
  var descriptors = utt.split('_');
  var objProperties = object.split('_').slice(0,3);
  var matches = _.every(map(function(descriptor) {
    return _.includes(objProperties, descriptor);
  }, descriptors));
  return matches;
};

// cost of producing utterance is sum of costs of the component words
var uttCost = function(utt) {
  var descriptors = utt.split('_');
  return sum(map(function(descriptor) {
    return descriptor == 'null' ? 0 : params[descriptor + 'Cost'];
  }, descriptors));
};

// derives the speaker's perspective from the listener's visible context
var getSpeakerView = function(listenerContext) {
  var hiddenObj = find(function(x) {return last(x) == 'private';}, listenerContext);
  return remove(hiddenObj, listenerContext);
};

// L0 interprets utterance literally using fixed mixture of perspectives
var L0 = function(utt, context, weighting) {
  return Infer({method: 'enumerate'}, function() {
    var perspective = flip(weighting) ? 'other' : 'own';
    var partnerContext = perspective == 'own' ? context : getSpeakerView(context);
    var object = uniformDraw(partnerContext);
    condition(meaning(utt, object));
    return object;
  });
};

// S1 selects utterance using fixed mixture of perspectives
// (given belief about L0's mixture)
var S1 = function(target, context, ownWeighting, partnerWeighting) {
  return Infer({method: 'enumerate'}, function() {
    var utt = uniformDraw(possibleUtterances);
    
    // Weight proportional to utility
    var perspective = flip(ownWeighting) ? 'other' : 'own';
    var informativity = (
      perspective == 'own' ? L0(utt, context, partnerWeighting).score(target) :
        expectation(possibleObjects, function(obj) {
          var possibleListenerView = context.concat(obj + '_private');
          return (L0(utt, possibleListenerView, partnerWeighting).score(target));
        })
    );

    factor(params.alpha * (informativity - uttCost(utt,params)));
    return utt;
  });
};

// meta-cognitive resource-rational speaker selects optimal mixture
// weight, marginalizing over uncertainty about listener's weight
var RR_speaker = function(target, context) {
  return Infer({method: 'enumerate'}, function() {
    var ownWeighting = sample(possibleWeightings);

    // Marginalize over partner's mixture weight and possible objects
    // in partner's view
    var utility = expectation(possibleObjects, function(obj) {
      return expectation(possibleWeightings, function(partnerWeighting) {
        var hypotheticalSelf = S1(target, context, ownWeighting, partnerWeighting);
        var utt = MAP(hypotheticalSelf)['val'];
        var possibleListenerView = context.concat(obj + '_2');
        return L0(utt, possibleListenerView, partnerWeighting).score(target);
      });
    });

    // putting more weight on partner's perspective is costly
    factor(utility - ownWeighting * params.perspectiveCost);
    return ownWeighting;
  });
}

var context = contexts['diff_shape'];
console.log(JSON.stringify(RR_speaker('texture_color_shape', context)));
